{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "914ad902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e1cb4",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning: Regression and Classification \n",
    "\n",
    "## Week 2: Regression with multiple input variables\n",
    "\n",
    "### Multiple Features\n",
    "\n",
    "We previoulsy considered a single feature in our linear regression.\n",
    "\n",
    "We can expand to multiple features, *e.g.* instead of a single feature, $x$, we\n",
    "have multiple features, $x_1, x_2, ...$.\n",
    "\n",
    "We use $j$ to index the features, *i.e* the $j^{th}$ feature is $x_j$.\n",
    "\n",
    "For a given example, $i$, we thus replace our previous scalar, $x^{(i)}$, with\n",
    "a **row vector**, $\\vec{x}^{(i)}$.\n",
    "\n",
    "The linear formula thus changes from $f_{w,b}(x) = wx + b$ to:\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "Note that we are computing the dot product of $\\vec{w}$ and $\\vec{x}$. For these\n",
    "two row vectors, this is equivalent to pairwise multiplication of the elements\n",
    "of $\\vec{w}$ and $\\vec{x}$.\n",
    "\n",
    "Note that $b$ remains a scalar.\n",
    "\n",
    "We now have the formula for **multiple linear regression** (which is not the\n",
    "same thing as *multivariate regression*).\n",
    "\n",
    "We now have the cost function:\n",
    "$$J(\\vec{w}, b)$$\n",
    "\n",
    "So the gradient descent algorithm becomes:\n",
    "$$w_j = w_j - \\alpha\\frac{\\partial}{\\partial w_j} J (\\vec{w}, b)$$\n",
    "$$b = b - \\alpha\\frac{\\partial}{\\partial b} J (\\vec{w}, b)$$\n",
    "\n",
    "Substituting in the formula for $J$ and differentiating we get the following\n",
    "algorithm for **gradient descent for multiple linear regression**:\n",
    "\n",
    "repeat{\n",
    "$$w_j = w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_j^{(i)} \\quad \\forall j \\in (1, 2, ..., n)$$\n",
    "$$b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})$$\n",
    "... simultaneously update $w_j \\forall j \\in (1, 2, ..., n)$ and $b$ \n",
    "}\n",
    "\n",
    "Note that the formula for $w_j$ contains both $\\vec{x}^{(i)}$ and $x_j^{(i)}$\n",
    "\n",
    "### An alternative to gradient descent\n",
    "The **normal equation** can be used as an alternative method to solve for\n",
    "$\\vec{w}$ and $b$ for linear regression *only* (it is not suitable for other\n",
    "applications). The normal equation has the benefit of not requring iterations,\n",
    "but it can be slow when the number of feaures is large (*e.g.* above 10,000).\n",
    "It will not be discussed further, but it may be used under the hood when using\n",
    "a mature library for performing linear regression.\n",
    "\n",
    "We will now implement multiple regression in python for the training data below:\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms | Number of floors | Age of Home | Price (1000s dollars) |\n",
    "|-----|---|---|----|-----|\n",
    "|2104 | 5 | 1 | 45 | 460 |\n",
    "|1416 | 3 | 2 | 40 | 232 |\n",
    "| 852 | 2 | 1 | 35 | 178 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.float64:\n",
    "    \"\"\"Get f(w, b).\"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "\n",
    "def cost(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float) -> np.float64:\n",
    "    \"\"\"Compute mean squared error.\"\"\"\n",
    "    return np.sum((predict(X, w, b) - y) ** 2) / (2 * X.shape[0])\n",
    "\n",
    "\n",
    "def gradient(\n",
    "    X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float\n",
    ") -> tuple[np.ndarray, np.float64]:\n",
    "    m = X.shape[0]\n",
    "    error_vector = predict(X, w, b) - y\n",
    "    dj_dw = np.dot(X.T, error_vector) / m\n",
    "    dj_db = np.sum(error_vector) / m\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    w0: np.ndarray,\n",
    "    b0: float,\n",
    "    alpha: float,\n",
    "    max_iters: int,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Perform iterative gradient descent.\"\"\"\n",
    "    w = copy.deepcopy(w0)  # avoid modifying external w0.\n",
    "    b = b0\n",
    "    for _ in range(max_iters):\n",
    "        dj_dw, dj_db = gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "    return w, b\n",
    "\n",
    "\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "alpha = 5e-7\n",
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94730b",
   "metadata": {},
   "source": [
    "### Feature scaling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
