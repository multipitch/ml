{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914ad902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779e1cb4",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning: Regression and Classification \n",
    "\n",
    "## Week 2: Regression with multiple input variables\n",
    "\n",
    "### Multiple Features\n",
    "\n",
    "We previoulsy considered a single feature in our linear regression.\n",
    "\n",
    "We can expand to multiple features, *e.g.* instead of a single feature, $x$, we\n",
    "have multiple features, $x_1, x_2, ...$.\n",
    "\n",
    "We use $j$ to index the features, *i.e* the $j^{th}$ feature is $x_j$.\n",
    "\n",
    "For a given example, $i$, we thus replace our previous scalar, $x^{(i)}$, with\n",
    "a **row vector**, $\\vec{x}^{(i)}$.\n",
    "\n",
    "The linear formula thus changes from $f_{w,b}(x) = wx + b$ to:\n",
    "$$f_{\\vec{w},b}(\\vec{x}) = \\vec{w}\\cdot\\vec{x} + b$$\n",
    "Note that we are computing the dot product of $\\vec{w}$ and $\\vec{x}$. For these\n",
    "two row vectors, this is equivalent to pairwise multiplication of the elements\n",
    "of $\\vec{w}$ and $\\vec{x}$.\n",
    "\n",
    "Note that $b$ remains a scalar.\n",
    "\n",
    "We now have the formula for **multiple linear regression** (which is not the\n",
    "same thing as *multivariate regression*).\n",
    "\n",
    "We now have the cost function:\n",
    "$$J(\\vec{w}, b)$$\n",
    "\n",
    "So the gradient descent algorithm becomes:\n",
    "$$w_j = w_j - \\alpha\\frac{\\partial}{\\partial w_j} J (\\vec{w}, b)$$\n",
    "$$b = b - \\alpha\\frac{\\partial}{\\partial b} J (\\vec{w}, b)$$\n",
    "\n",
    "Substituting in the formula for $J$ and differentiating we get the following\n",
    "algorithm for **gradient descent for multiple linear regression**:\n",
    "\n",
    "repeat{\n",
    "$$w_j = w_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})x_j^{(i)} \\quad \\forall j \\in (1, 2, ..., n)$$\n",
    "$$b = b - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}\n",
    "(f_{\\vec{w},b}(\\vec{x}^{(i)})-y^{(i)})$$\n",
    "... simultaneously update $w_j \\forall j \\in (1, 2, ..., n)$ and $b$ \n",
    "}\n",
    "\n",
    "Note that the formula for $w_j$ contains both $\\vec{x}^{(i)}$ and $x_j^{(i)}$\n",
    "\n",
    "### An alternative to gradient descent\n",
    "The **normal equation** can be used as an alternative method to solve for\n",
    "$\\vec{w}$ and $b$ for linear regression *only* (it is not suitable for other\n",
    "applications). The normal equation has the benefit of not requring iterations,\n",
    "but it can be slow when the number of feaures is large (*e.g.* above 10,000).\n",
    "It will not be discussed further, but it may be used under the hood when using\n",
    "a mature library for performing linear regression.\n",
    "\n",
    "We will now implement multiple regression in python for the training data below:\n",
    "\n",
    "| Size (sqft) | Number of Bedrooms | Number of floors | Age of Home | Price (1000s dollars) |\n",
    "|-----|---|---|----|-----|\n",
    "|2104 | 5 | 1 | 45 | 460 |\n",
    "|1416 | 3 | 2 | 40 | 232 |\n",
    "| 852 | 2 | 1 | 35 | 178 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68a277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([459.99999762, 231.99999837, 177.99999899])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
    "    \"\"\"Get f(w, b) for an array of X-values.\n",
    "\n",
    "    Args:\n",
    "        X: np.ndarray: A 2D matrix of input variables with dimensions\n",
    "          m x n, where m is the number of rows (observations) and n is\n",
    "          the number of columns (features).\n",
    "        w: np.ndarray: A vector of weights, of length n, where n is the\n",
    "          number of features. \n",
    "        b: float: Bias.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A vector of length m of predicted values.\n",
    "    \"\"\"\n",
    "    return np.dot(X, w) + b\n",
    "\n",
    "\n",
    "def cost(X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float) -> np.float64:\n",
    "    \"\"\"Compute mean squared error cost function.\n",
    "\n",
    "    Args:\n",
    "        X: np.ndarray: A 2D matrix of input variables with dimensions\n",
    "          m x n, where m is the number of rows (observations) and n is\n",
    "          the number of columns (features).\n",
    "        y: np.ndarray: A vector of output variables of length m.\n",
    "        w: np.ndarray: A vector of weights, of length n, where n is the\n",
    "          number of features. \n",
    "        b: float: Bias.\n",
    "\n",
    "    Returns:\n",
    "        np.float64: The MSE cost.\n",
    "    \"\"\"\n",
    "    return np.sum((predict(X, w, b) - y) ** 2) / (2 * X.shape[0])\n",
    "\n",
    "\n",
    "def gradient(\n",
    "    X: np.ndarray, y: np.ndarray, w: np.ndarray, b: float\n",
    ") -> tuple[np.ndarray, np.float64]:\n",
    "    \"\"\"Get partial differentials of J_(w, b)(X) with respect to w and b.\n",
    "\n",
    "    Args:\n",
    "        X: np.ndarray: A 2D matrix of input variables with dimensions\n",
    "          m x n, where m is the number of rows (observations) and n is\n",
    "          the number of columns (features).\n",
    "        y: np.ndarray: A vector of output variables of length m.\n",
    "        w: np.ndarray: A vector of weights, of length n, where n is the\n",
    "          number of features. \n",
    "        b: float: Bias.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.float64]: A tuple of length 2 containing\n",
    "          the partial derivatives of the MSE cost. The first item in\n",
    "          the tuple is a vector of length n of the partial derivatives\n",
    "          with respect to all the weights in w, and the second item is \n",
    "          the partial derivative with respect to the bias, b.\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    error_vector = predict(X, w, b) - y\n",
    "    dj_dw = np.dot(X.T, error_vector) / m\n",
    "    dj_db = np.sum(error_vector) / m\n",
    "    return dj_dw, dj_db\n",
    "\n",
    "\n",
    "def gradient_descent(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    w0: np.ndarray,\n",
    "    b0: float,\n",
    "    alpha: float,\n",
    "    max_iters: int,\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"Perform iterative gradient descent.\n",
    "\n",
    "    Args:\n",
    "        X: np.ndarray: A 2D matrix of input variables with dimensions\n",
    "          m x n, where m is the number of rows (observations) and n is\n",
    "          the number of columns (features).\n",
    "        y: np.ndarray: A vector of output variables of length m.\n",
    "        w0: np.ndarray: A vector of length n of initial weights (slopes)\n",
    "          for each feature.\n",
    "        b0: float: Bias (intercept).\n",
    "        alpha: float: Learning rate.\n",
    "        max_iters: Maximum number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple[np.ndarray, np.float64]: Updated values for w and b.\n",
    "    \"\"\"\n",
    "    w = copy.deepcopy(w0)  # avoid modifying external w0.\n",
    "    b = b0\n",
    "    for _ in range(max_iters):\n",
    "        dj_dw, dj_db = gradient(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "    return w, b\n",
    "\n",
    "\n",
    "X_train = np.array([[2104, 5, 1, 45], [1416, 3, 2, 40], [852, 2, 1, 35]])\n",
    "y_train = np.array([460, 232, 178])\n",
    "b_init = 785.1811367994083\n",
    "w_init = np.array([0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "alpha = 5e-7\n",
    "max_iters = 1000\n",
    "predict(X_train, w_init, b_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c94730b",
   "metadata": {},
   "source": [
    "### Feature scaling\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
